{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pretraining BERT Models**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this hands-on lab, you will learn how to build a BERT model from scratch using PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Background\">Background</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Introduction-to-pretraining\">Introduction to pretraining</a></li>\n",
    "            <li><a href=\"#Pretraining-objectives\">Pretraining objectives</a></li>\n",
    "            <li><a href=\"#Pretraining-a-BERT-model\">Pretraining a BERT model</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Loading-data\">Loading data</a></li>\n",
    "    <li><a href=\"#Model-creation\">Model creation</a></li>\n",
    "    <li><a href=\"#Evaluation\">Evaluation</a></li>\n",
    "    <li><a href=\"#Training\">Training</a></li>\n",
    "    <li><a href=\"#Inference\">Inference</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Exercises\">Exercises</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Exercise-1:-Next-Sentence-Prediction-(NSP)-with-BERT\">Exercise 1: Next Sentence Prediction (NSP) with BERT</a></li>\n",
    "            <li><a href=\"#Exercise-2:-Masked-Language-Modeling-(MLM)-with-BERT\">Exercise 2: Masked Language Modeling (MLM) with BERT</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this interactive guide, you will delve into the core components of encoder models, with a spotlight on 'Baby BERT', a streamlined variant of the BERT model. This notebook aims to:\n",
    "\n",
    "- **Demystify tokenization**: Introduce the critical first step in processing text for Natural Language Processing (NLP) — tokenization. We'll learn how to convert raw text into a format that's amenable to machine processing.\n",
    "- **Decode encoder models**: Shed light on the structure and function of encoder models, particularly how they capture and process the complexities of language.\n",
    "- **Pretraining dynamics**: Walk through the pretraining phase of 'Baby BERT' on a bespoke dataset, showcasing how language models are equipped to understand context and meaning.\n",
    "- **Task-specific proficiency**: Assess 'Baby BERT's capabilities in handling sentence sequencing and word prediction challenges through Next Sentence Prediction (NSP) and Masked Language Modeling (MLM).\n",
    "- **Performance analysis**: Evaluate how well our model performs these tasks, providing insights into its linguistic comprehension and ability to predict accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n",
    "> Note: After executing the 'pip install' cell restart the kernel and execute the subsequent cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pandas==2.2.1\n",
      "  Downloading pandas-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas==2.2.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas==2.2.1) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.2.1)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.1) (1.17.0)\n",
      "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m285.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m198.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.26.4 pandas-2.2.1 tzdata-2025.2\n",
      "Collecting torch==2.3.0\n",
      "  Downloading torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchtext==0.18.0\n",
      "  Downloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
      "Collecting transformers==4.35.2\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "Collecting filelock (from torch==2.3.0)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.3.0) (4.12.2)\n",
      "Collecting sympy (from torch==2.3.0)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.3.0)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.3.0) (3.1.5)\n",
      "Collecting fsspec (from torch==2.3.0)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from torchtext==0.18.0) (4.67.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from torchtext==0.18.0) (2.32.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchtext==0.18.0) (1.26.4)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.2)\n",
      "  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers==4.35.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.35.2) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.35.2)\n",
      "  Downloading regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.35.2)\n",
      "  Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers==4.35.2)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->torchtext==0.18.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->torchtext==0.18.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->torchtext==0.18.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->torchtext==0.18.0) (2024.12.14)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.3.0)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m188.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m803.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m209.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m493.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m142.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m622.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m189.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m425.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (802 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.0/802.0 kB\u001b[0m \u001b[31m421.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m588.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m517.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m692.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m714.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m737.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m158.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: mpmath, sympy, safetensors, regex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, hf-xet, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, torchtext\n",
      "Successfully installed filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.1 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 regex-2025.9.18 safetensors-0.6.2 sympy-1.14.0 tokenizers-0.15.2 torch-2.3.0 torchtext-0.18.0 transformers-4.35.2\n",
      "Collecting matplotlib==3.9.0\n",
      "  Downloading matplotlib-3.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.9.0)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.9.0)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.9.0)\n",
      "  Downloading fonttools-4.60.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib==3.9.0)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.0) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib==3.9.0)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib==3.9.0)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib==3.9.0) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.9.0) (1.17.0)\n",
      "Downloading matplotlib-3.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m505.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m683.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m590.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.0 kiwisolver-1.4.9 matplotlib-3.9.0 pillow-11.3.0 pyparsing-3.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir numpy==1.26.4 pandas==2.2.1\n",
    "!pip install --no-cache-dir torch==2.3.0 torchtext==0.18.0 transformers==4.35.2\n",
    "!pip install --no-cache-dir matplotlib==3.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "_It is recommended you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchtext.vocab import Vocab,build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import IMDB\n",
    "import random\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Background\n",
    "### Introduction to pretraining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining involves training a model on a large corpus of unlabeled text to capture general language patterns and semantic relationships. Pretrained models can then be fine-tuned on specific downstream NLP tasks, such as sentiment analysis, question answering, or machine translation.\n",
    "\n",
    "The motivation behind pretraining transformers is to address the limitations of traditional approaches that require significant amounts of labeled data for each specific task. Pretraining leverages the abundance of unlabeled text data available on the internet and facilitates transfer learning, where knowledge learned from one task can be transferred to aid in solving other related tasks.\n",
    "\n",
    "Pretraining objectives play a crucial role in training transformers. For example, masked language modeling (MLM) involves randomly masking some words in a sentence and training the model to predict the masked words based on the surrounding context. This objective helps the model learn contextual understanding and fill in missing information. Another objective called next sentence prediction (NSP) involves predicting whether two sentences are consecutive or randomly chosen from the corpus, enabling the model to learn sentence-level relationships.\n",
    "\n",
    "In the next sections of this lab, you will delve deeper into pretraining objectives. By the end of this lab, you will have a solid understanding of pretraining tasks for BERT models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pretraining objectives\n",
    "\n",
    "Pretraining objectives are crucial components of the pretraining process for transformers. These objectives define the tasks that the model is trained on during the pretraining phase, allowing it to learn meaningful contextual representations of language. Two commonly used pretraining objectives are masked language modeling (MLM) and next sentence prediction (NSP).\n",
    "\n",
    "1. Masked Language Modeling (MLM):\n",
    "   Masked language modeling involves randomly masking some words in a sentence and training the model to predict the masked words based on the context provided by the surrounding words(i.e., words that appear either before or after the masked word). The objective is to enable the model to learn contextual understanding and fill in missing information.\n",
    "\n",
    "   Here's how MLM works:\n",
    "   - Given an input sentence, a certain percentage of the words are randomly chosen and replaced with a special [MASK] token.\n",
    "   - The model's task is to predict the original words that were masked, given the context of the surrounding words.\n",
    "   - During training, the model learns to understand the relationship between the masked words and the rest of the sentence, effectively capturing the contextual information.\n",
    "\n",
    "2. Next Sentence Prediction (NSP):\n",
    "   Next sentence prediction involves training the model to predict whether two sentences are consecutive in the original text or randomly chosen from the corpus. This objective helps the model learn sentence-level relationships and understand the coherence between sentences.\n",
    "\n",
    "   Here's how NSP works:\n",
    "   - Given a pair of sentences, the model is trained to predict whether the second sentence follows the first sentence in the original text or if it is randomly selected from the corpus.\n",
    "   - The model learns to capture the relationships between sentences and understand the flow of information in the text.\n",
    "\n",
    "   NSP is particularly useful for tasks that involve understanding the relationship between multiple sentences, such as question answering or document classification. By training the model to predict the coherence of sentence pairs, it learns to capture the semantic connections between them.\n",
    "\n",
    "It's important to note that different pretrained models may use variations or combinations of these objectives, depending on the specific architecture and training setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining a BERT model\n",
    "Pretraining a BERT(Bidirectional Encoder Representations from Transformers) model is a complex and time-consuming process that requires a large corpus of unlabeled text data and significant computational resources. However, you provide with a simplified exercise to demonstrate the steps involved in pretraining a BERT model using the Masked Language Modeling (MLM) as well as the Next Sentence Prediction (NSP) objectives.\n",
    "\n",
    "You will be instructed to:\n",
    "- Create train and test dataloaders from dataset\n",
    "- Pretrain BERT using an MLM task\n",
    "- Pretrain BERT using an NSP task\n",
    "- Evaluate the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "Let's load the CSV files created in the data preparation lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove any earlier instance of the dataset loaded\n",
    "if [ -d \"bert_dataset\" ]; then\n",
    "    rm -rf bert_dataset\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-24 23:12:59--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 88958506 (85M) [application/zip]\n",
      "Saving to: ‘BERT_dataset.zip’\n",
      "\n",
      "BERT_dataset.zip    100%[===================>]  84.84M  56.1MB/s    in 1.5s    \n",
      "\n",
      "2025-09-24 23:13:01 (56.1 MB/s) - ‘BERT_dataset.zip’ saved [88958506/88958506]\n",
      "\n",
      "Archive:  BERT_dataset.zip\n",
      "   creating: bert_dataset/\n",
      "  inflating: bert_dataset/.DS_Store  \n",
      "  inflating: bert_dataset/bert_train_data.csv  \n",
      "  inflating: bert_dataset/bert_test_data_sampled.csv  \n",
      "  inflating: bert_dataset/bert_test_data.csv  \n",
      "  inflating: bert_dataset/bert_train_data_sampled.csv  \n"
     ]
    }
   ],
   "source": [
    "!wget -O BERT_dataset.zip https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
    "!unzip BERT_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can create a torch Dataset using the CSV file you just created:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCSVDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.data = pd.read_csv(filename)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        try:\n",
    "            \n",
    "            bert_input = torch.tensor(json.loads(row['BERT Input']), dtype=torch.long)\n",
    "            bert_label = torch.tensor(json.loads(row['BERT Label']), dtype=torch.long)\n",
    "            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')], dtype=torch.long)\n",
    "            is_next = torch.tensor(row['Is Next'], dtype=torch.long)\n",
    "            original_text = row['Original Text']  # If you want to use it\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for row {idx}: {e}\")\n",
    "            print(\"BERT Input:\", row['BERT Input'])\n",
    "            print(\"BERT Label:\", row['BERT Label'])\n",
    "            # Handle the error, e.g., by skipping this row or using default values\n",
    "            return None  # or some default values\n",
    "\n",
    "            # Tokenizing the original text with BERT\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            original_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_input['input_ids'].squeeze()\n",
    "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
    "\n",
    "        return(bert_input, bert_label, segment_label, is_next, input_ids, attention_mask, original_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a collate function that applies transformations on batches of data iterator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "def collate_batch(batch):\n",
    "\n",
    "   \n",
    "    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch,input_ids_batch,attention_mask_batch,original_text_battch = [], [], [], [],[],[],[]\n",
    "\n",
    "    for bert_input, bert_label, segment_label, is_next,input_ids,attention_mask,original_text in batch:\n",
    "        # Convert each sequence to a tensor and append to the respective list\n",
    "        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
    "        bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
    "        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
    "        is_nexts_batch.append(is_next)\n",
    "        input_ids_batch.append(input_ids)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "        original_text_battch.append(original_text)\n",
    "\n",
    "    # Pad the sequences in the batch\n",
    "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an arbitrary batch size, you can create train and test dataloaders:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset_path = './bert_dataset/bert_train_data.csv'\n",
    "test_dataset_path = './bert_dataset/bert_test_data.csv'\n",
    "\n",
    "train_dataset = BERTCSVDataset(train_dataset_path)\n",
    "test_dataset = BERTCSVDataset(test_dataset_path)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "\n",
    "In BERT, positional embedding, token embedding, and segment embedding are three types of embeddings used to represent the input tokens in the model.\n",
    "\n",
    "1. Token Embedding: Token embedding is the initial representation of each token in a BERT model. It maps each token to a dense vector representation of a fixed size, typically referred to as the embedding size. The token embedding layer in BERT learns the contextual representations of the input tokens. These embeddings capture the semantic meaning of the tokens and their relationships with other tokens in the context.\n",
    "\n",
    "2. Positional Embedding: BERT is a transformer-based model that processes the input tokens in parallel. However, since transformers don't inherently capture the order of tokens, positional embedding is used to inject positional information into the model. It adds a vector representation to each token that encodes its position in the input sequence. The positional embedding allows BERT to understand the sequential order of the tokens and capture their relative positions.\n",
    "\n",
    "\n",
    "3. Segment Embedding: BERT can handle sentence pairs or sequences that have distinct segments or parts. To differentiate between different segments, such as sentences or document sections, segment embedding is used. It assigns a unique vector representation to each segment or part of the input. The segment embeddings help BERT understand the relationships between different segments and capture the context within and between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Define the PositionalEncoding class as a PyTorch module for adding positional information to token embeddings\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a positional encoding matrix as per the Transformer paper's formula\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        # Apply the positional encodings to the input token embeddings\n",
    "\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding (nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size ,dropout=0.1,train=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = TokenEmbedding( vocab_size,emb_size )\n",
    "        self.positional_encoding = PositionalEncoding(emb_size,dropout)\n",
    "        self.segment_embedding = nn.Embedding(3, emb_size)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, bert_inputs, segment_labels=False):\n",
    "        my_embeddings=self.token_embedding(bert_inputs)\n",
    "        if self.train:\n",
    "          x = self.dropout(my_embeddings + self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))\n",
    "        else:\n",
    "          x = my_embeddings + self.positional_encoding(my_embeddings)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define a complete BERT model with the following key components:\n",
    "\n",
    "1. Initialization: The `BERT` class is defined as a subclass of `torch.nn.Module`. It initializes the BERT model with parameters such as vocabulary size, model dimension, number of layers, number of attention heads, and dropout rate.\n",
    "\n",
    "2. Embedding Layer: The BERT model includes an embedding layer that combines token embeddings and segment embeddings using the `BERTEmbedding` class.\n",
    "\n",
    "3. Transformer Encoder: Transformer Encoder layers are used to encode the input embeddings. The number of layers, attention heads, dropout rate, and model dimension are specified based on the defined parameters.\n",
    "\n",
    "4. Next Sentence Prediction: The model has a linear layer for Next Sentence Prediction. It takes the output from the Transformer encoder and predicts the relationship between two consecutive sentences, classifying them into two classes.\n",
    "\n",
    "5. Masked Language Modeling: The model also includes a linear layer for Masked Language Modeling. It predicts the masked tokens in the input sequence by taking the output from the Transformer encoder and making predictions across the vocabulary.\n",
    "\n",
    "6. Forward Pass: The `forward` method defines the forward pass of the BERT model. It takes input tokens (`bert_inputs`) and segment labels (`segment_labels`) and returns predictions for Next Sentence Prediction and Masked Language Modeling tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an example input, let's break down the embedding into its three essential components: token embedding, positional encoding and segment embedding to comprehend the process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=147161\n",
    "batch = 2\n",
    "count = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# load sample batches from dataloader\n",
    "for batch in train_dataloader:\n",
    "    bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,     3,  1901,    14,   162,    12,    30,   218,   173,    63,\n",
       "            3,   234,   517,     3,    52,   162,    12,    30,  1149,     3,\n",
       "           18,    13,    32,     5,   163,    15,    17,    24,     7,     5,\n",
       "          234,    13,   407,    15,  3256,     7,    14,   174,     5,    24,\n",
       "           66,    62,  1412,     6,     2,    17,   185,    10,     5,    24,\n",
       "           89,  4012,   142,    21,    60,    21,     5,    91,   528,     7,\n",
       "            9,    96,   792,    22,     9, 29432,  1544,    22,   257,     6,\n",
       "            2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pick a sample input\n",
    "bert_inputs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_labels[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of token embeddings: torch.Size([90, 2, 10])\n",
      "Token Embeddings for the 0th token of the first sample: tensor([ 5.2222,  0.0915,  3.5805, -0.8280,  4.8769, -0.0620,  3.9806,  0.8648,\n",
      "         0.5627, -3.9214], grad_fn=<SliceBackward0>)\n",
      "Token Embeddings for the 1th token of the first sample: tensor([ 1.8530, -3.6614, -1.9674,  3.0242, -2.1179, -3.3241,  0.3980,  1.2189,\n",
      "         4.4026,  0.8051], grad_fn=<SliceBackward0>)\n",
      "Token Embeddings for the 2th token of the first sample: tensor([ 1.4795,  2.9125,  0.3651,  1.1453, -4.0421, -0.2826,  3.0219,  5.5880,\n",
      "        -2.3550,  4.3411], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the TokenEmbedding \n",
    "token_embedding = TokenEmbedding(VOCAB_SIZE, emb_size=EMBEDDING_DIM )\n",
    "\n",
    "# Get the token embeddings for a sample input\n",
    "t_embeddings = token_embedding(bert_inputs)\n",
    "#Each token is transformed into a tensor of size emb_size\n",
    "print(f\"Dimensions of token embeddings: {t_embeddings.size()}\") # Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
    "#Check the embedded vectors for first 3 tokens of the first sample in the batch\n",
    "# you get embeddings[i,0,:] where i refers to the i'th token of the first sample in the batch (b=0)\n",
    "for i in range(3):\n",
    "    print(f\"Token Embeddings for the {i}th token of the first sample: {t_embeddings[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of positionally encoded tokens: torch.Size([90, 2, 10])\n",
      "Positional Embeddings for the 0th token of the first sample: tensor([ 5.2222,  1.0915,  3.5805,  0.1720,  4.8769,  0.9380,  3.9806,  1.8648,\n",
      "         0.5627, -2.9214], grad_fn=<SliceBackward0>)\n",
      "Positional Embeddings for the 1th token of the first sample: tensor([ 2.6945, -3.1211, -1.8096,  4.0116, -2.0927, -2.3244,  0.4020,  2.2189,\n",
      "         4.4032,  1.8051], grad_fn=<SliceBackward0>)\n",
      "Positional Embeddings for the 2th token of the first sample: tensor([ 2.3888,  2.4964,  0.6768,  2.0955, -3.9919,  0.7161,  3.0299,  6.5880,\n",
      "        -2.3538,  5.3411], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "positional_encoding = PositionalEncoding(emb_size=EMBEDDING_DIM,dropout=0)\n",
    "\n",
    "# Apply positional encoding to token embeddings\n",
    "p_embedding = positional_encoding(t_embeddings)\n",
    "\n",
    "print(f\"Dimensions of positionally encoded tokens: {p_embedding.size()}\")# Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
    "#Check the positional encoded vectors for first 3 tokens of the first sample in the batch\n",
    "# you get encoded_tokens[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch \n",
    "for i in range(3):\n",
    "    print(f\"Positional Embeddings for the {i}th token of the first sample: {p_embedding[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of segment embedding: torch.Size([90, 2, 10])\n",
      "Segment Embeddings for the 0th token of the first sample: tensor([-0.8415, -1.4436, -0.1274,  0.8046,  0.2380, -1.2740,  0.0677, -0.3220,\n",
      "         0.0391, -0.4562], grad_fn=<SliceBackward0>)\n",
      "Segment Embeddings for the 1th token of the first sample: tensor([-0.8415, -1.4436, -0.1274,  0.8046,  0.2380, -1.2740,  0.0677, -0.3220,\n",
      "         0.0391, -0.4562], grad_fn=<SliceBackward0>)\n",
      "Segment Embeddings for the 2th token of the first sample: tensor([-0.8415, -1.4436, -0.1274,  0.8046,  0.2380, -1.2740,  0.0677, -0.3220,\n",
      "         0.0391, -0.4562], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "segment_embedding = nn.Embedding(3, EMBEDDING_DIM)\n",
    "s_embedding = segment_embedding(segment_labels)\n",
    "print(f\"Dimensions of segment embedding: {s_embedding.size()}\")# Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
    "#Check the Segment Embedding vectors for first 3 tokens of the first sample in the batch\n",
    "# you get segment_embedded[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch \n",
    "for i in range(3):\n",
    "    print(f\"Segment Embeddings for the {i}th token of the first sample: {s_embedding[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of token + position + segment encoded tokens: torch.Size([90, 2, 10])\n",
      "BERT_Embedding for 0th token: tensor([ 9.6028, -0.2605,  7.0337,  0.1487,  9.9918, -0.3980,  8.0290,  2.4076,\n",
      "         1.1645, -7.2990], grad_fn=<SliceBackward0>)\n",
      "BERT_Embedding for 1th token: tensor([ 3.7060, -8.2260, -3.9043,  7.8404, -3.9726, -6.9225,  0.8678,  3.1158,\n",
      "         8.8449,  2.1539], grad_fn=<SliceBackward0>)\n",
      "BERT_Embedding for 2th token: tensor([ 3.0268,  3.9653,  0.9146,  4.0454, -7.7960, -0.8406,  6.1195, 11.8540,\n",
      "        -4.6697,  9.2260], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Create the combined embedding vectors\n",
    "bert_embeddings = t_embeddings + p_embedding + s_embedding\n",
    "print(f\"Dimensions of token + position + segment encoded tokens: {bert_embeddings.size()}\")\n",
    "#Check the BERT Embedding vectors for first 3 tokens of the first sample in the batch\n",
    "# you get bert_embeddings[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch \n",
    "for i in range(3):\n",
    "    print(f\"BERT_Embedding for {i}th token: {bert_embeddings[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        vocab_size: The size of the vocabulary.\n",
    "        d_model: The size of the embeddings (hidden size).\n",
    "        n_layers: The number of Transformer layers.\n",
    "        heads: The number of attention heads in each Transformer layer.\n",
    "        dropout: The dropout rate applied to embeddings and Transformer layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "\n",
    "        # Embedding layer that combines token embeddings and segment embeddings\n",
    "        self.bert_embedding = BERTEmbedding(vocab_size, d_model, dropout)\n",
    "\n",
    "        # Transformer Encoder layers\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Linear layer for Next Sentence Prediction\n",
    "        self.nextsentenceprediction = nn.Linear(d_model, 2)\n",
    "\n",
    "        # Linear layer for Masked Language Modeling\n",
    "        self.masked_language = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, bert_inputs, segment_labels):\n",
    "        \"\"\"\n",
    "        bert_inputs: Input tokens.\n",
    "        segment_labels: Segment IDs for distinguishing different segments in the input.\n",
    "        mask: Attention mask to prevent attention to padding tokens.\n",
    "\n",
    "        return: Predictions for next sentence task and masked language modeling task.\n",
    "        \"\"\"\n",
    "\n",
    "        padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
    "        # Generate embeddings from input tokens and segment labels\n",
    "        my_bert_embedding = self.bert_embedding(bert_inputs, segment_labels)\n",
    "\n",
    "        # Pass embeddings through the Transformer encoder\n",
    "        transformer_encoder_output = self.transformer_encoder(my_bert_embedding,src_key_padding_mask=padding_mask)\n",
    "\n",
    "\n",
    "        next_sentence_prediction = self.nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
    "        \n",
    "\n",
    "        # Masked Language Modeling: Predict all tokens in the sequence\n",
    "        masked_language = self.masked_language(transformer_encoder_output)\n",
    "\n",
    "        return  next_sentence_prediction, masked_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an instance of the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = 147161  # Replace VOCAB_SIZE with your vocabulary size\n",
    "d_model = EMBEDDING_DIM  # Replace EMBEDDING_DIM with your embedding dimension\n",
    "n_layers = 2  # Number of Transformer layers\n",
    "initial_heads = 12 # Initial number of attention heads\n",
    "initial_heads = 2\n",
    "# Ensure the number of heads is a factor of the embedding dimension\n",
    "heads = initial_heads - d_model % initial_heads\n",
    "\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Create an instance of the BERT model\n",
    "model = BERT(vocab_size, d_model, n_layers, heads, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 90])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
    "padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 2, 10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "# Pass embeddings through the Transformer encoder\n",
    "transformer_encoder_output = transformer_encoder(bert_embeddings,src_key_padding_mask=padding_mask)\n",
    "transformer_encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSP Output Shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "nextsentenceprediction = nn.Linear(d_model, 2)\n",
    "nsp = nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
    "#logits for NSP task\n",
    "print(f\"NSP Output Shape: {nsp.shape}\")  # Expected shape: (batch_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM Output Shape: torch.Size([90, 2, 147161])\n"
     ]
    }
   ],
   "source": [
    "masked_language = nn.Linear(d_model, vocab_size)\n",
    "# Masked Language Modeling: Predict all tokens in the sequence\n",
    "mlm = masked_language(transformer_encoder_output)\n",
    "#logits for MLM task\n",
    "print(f\"MLM Output Shape: {mlm.shape}\")  # Expected shape: (seq_length, batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "After creating the BERT model, the next step is training and evaluating its performance. To facilitate this, an `evaluate` function is defined with the following steps:\n",
    "\n",
    "1. Loss Function: The CrossEntropyLoss function is defined to calculate the loss between predicted and actual values.\n",
    "\n",
    "2. Function Arguments: The function takes arguments including the dataloader, model, loss function, and device.\n",
    "\n",
    "3. Evaluation Mode: The BERT model is put into evaluation mode using `model.eval()`, disabling dropout and training-specific behaviors. Variables are initialized to track the total loss, total next sentence loss, total mask loss, and total number of batches.\n",
    "\n",
    "4. Evaluation Loop: The function iterates through the batches in the provided dataloader.\n",
    "\n",
    "5. Forward Pass: A forward pass is performed with the BERT model to obtain predictions for the next sentence and masked language tasks.\n",
    "\n",
    "6. Loss Calculation: The losses for the next sentence and masked language tasks are calculated, and then summed up to obtain the total loss for the batch.\n",
    "\n",
    "7. Average Loss Calculation: The average loss, average next sentence loss, and average mask loss are calculated by dividing the total losses by the total number of batches.\n",
    "\n",
    "\n",
    "The `evaluate` function is used not only for evaluating the BERT model's performance but also during the training phase to assess the model's progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX=0\n",
    "loss_fn_mlm = nn.CrossEntropyLoss(ignore_index=PAD_IDX)# The loss function must ignore PAD tokens and only calculates loss for the masked tokens\n",
    "loss_fn_nsp = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader=test_dataloader, model=model, loss_fn_mlm=loss_fn_mlm, loss_fn_nsp=loss_fn_nsp, device=device):\n",
    "    model.eval()  # Turn off dropout and other training-specific behaviors\n",
    "\n",
    "    total_loss = 0\n",
    "    total_next_sentence_loss = 0\n",
    "    total_mask_loss = 0\n",
    "    total_batches = 0\n",
    "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "        for batch in dataloader:\n",
    "            bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "\n",
    "            # Forward pass\n",
    "            next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
    "\n",
    "            # Calculate loss for next sentence prediction\n",
    "            # Ensure is_nexts is of the correct shape for CrossEntropyLoss\n",
    "            next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts.view(-1))\n",
    "\n",
    "            # Calculate loss for predicting masked tokens\n",
    "            # Flatten both masked_language predictions and bert_labels to match CrossEntropyLoss input requirements\n",
    "            mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
    "\n",
    "            # Sum up the two losses\n",
    "            loss = next_loss + mask_loss\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            else:\n",
    "                total_loss += loss.item()\n",
    "                total_next_sentence_loss += next_loss.item()\n",
    "                total_mask_loss += mask_loss.item()\n",
    "                total_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / (total_batches + 1)\n",
    "    avg_next_sentence_loss = total_next_sentence_loss / (total_batches + 1)\n",
    "    avg_mask_loss = total_mask_loss / (total_batches + 1)\n",
    "\n",
    "    print(f\"Average Loss: {avg_loss:.4f}, Average Next Sentence Loss: {avg_next_sentence_loss:.4f}, Average Mask Loss: {avg_mask_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The training process for the BERT model involves the following steps:\n",
    "\n",
    "1. Optimizer Definition: Before training starts, an optimizer is defined for training the BERT model. In this case, the Adam optimizer is used.\n",
    "\n",
    "2. Training Loop: Within each epoch, the training data is iterated through in batches.\n",
    "\n",
    "3. Forward Pass: For each batch, a forward pass is performed, where the BERT model predicts the next sentence and masked language tasks.\n",
    "\n",
    "4. Loss Calculation and Parameter Update: The loss is calculated based on the predicted and actual values. The model's parameters are then updated through backpropagation and gradient clipping.\n",
    "\n",
    "5. Epoch Evaluation: After each epoch, the average training loss is printed. The model's performance on the test set is evaluated. Additionally, the model is saved after each epoch.\n",
    "\n",
    "These steps are repeated for multiple epochs to train the BERT model and monitor its progress over time.\n",
    "\n",
    "**NOTE: The current DataLoader is quite huge, and it will take several hours for the model to train with such a huge dataset. Hence, below is the randomly sampled dataset (which is relatively small which still takes 1-2 hours to run) from IMDB to make the process faster. If you want to train the model on the entire dataset, skip the next cell and directly run the training cell.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "\n",
    "train_dataset_path = './bert_dataset/bert_train_data_sampled.csv'\n",
    "test_dataset_path = './bert_dataset/bert_test_data_sampled.csv'\n",
    "\n",
    "train_dataset = BERTCSVDataset(train_dataset_path)\n",
    "test_dataset = BERTCSVDataset(test_dataset_path)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "# Training loop setup\n",
    "num_epochs = 1\n",
    "total_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "# Define the number of warmup steps, e.g., 10% of total\n",
    "warmup_steps = int(total_steps * 0.1)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "# Lists to store losses for plotting\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n",
    "        bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
    "\n",
    "        next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts)\n",
    "        mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
    "\n",
    "        loss = next_loss + mask_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update the learning rate\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "        else:\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader) + 1\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch+1} - Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Evaluation after each epoch\n",
    "    eval_loss = evaluate(test_dataloader, model, loss_fn_nsp, loss_fn_mlm, device)\n",
    "    eval_losses.append(eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The below is a loss vs epoch plot, run the above code for more than one epoch to get a plot(currently the num_epochs is set to 1).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss values\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(range(1,num_epochs+1), train_losses, label=\"Training Loss\", color='blue')\n",
    "plt.scatter(range(1,num_epochs+1), eval_losses, label=\"Evaluation Loss\", color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Here you can load a model from a pt file that provide for your convenience.  So, If the model training on CPU takes too long, you can load this file and continue the lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model = BERT(vocab_size, d_model, n_layers, heads, dropout)  # Ensure these parameters match the original model's\n",
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/H04Cs7O75aOfmJ4YP2HdPw.pt'\n",
    "model.load_state_dict(torch.load('H04Cs7O75aOfmJ4YP2HdPw.pt',map_location=torch.device('cpu')))\n",
    "model.to(device)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of a pretrained BERT model in predicting whether a second sentence follows the first, a function called `predict_nsp` is defined. The function operates as follows:\n",
    "\n",
    "1. Tokenization: The input sentences are tokenized using the `tokenizer.encode_plus` method, which returns a dictionary of tokenized inputs. These tokenized inputs are then converted into tensors and moved to the appropriate device for processing.\n",
    "\n",
    "2. Prediction: The BERT model is utilized to make predictions by passing the token and segment tensors as input.\n",
    "\n",
    "3. Logits Manipulation: The first element of the logits tensor is selected and unsqueezed to add an extra dimension, resulting in a shape of `[1, 2]`.\n",
    "\n",
    "4. Probability and Prediction: The logits are passed through a softmax function to obtain probabilities, and the prediction is obtained by taking the argmax.\n",
    "\n",
    "5. Result Interpretation: The prediction is interpreted and returned as a string, indicating whether the second sentence follows the first or not.\n",
    "\n",
    "6. Example Usage: An example usage of the `predict_nsp` function is provided, demonstrating how two example sentences can be passed to the function along with a BERT model and tokenizer. The result is printed, indicating whether the second sentence follows the first based on the model's prediction.\n",
    "\n",
    "By utilizing the `predict_nsp` function, the performance of the pretrained BERT model in determining the relationship between two sentences can be assessed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second sentence follows the first\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer with the BERT model's vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "def predict_nsp(sentence1, sentence2, model, tokenizer):\n",
    "    # Tokenize sentences with special tokens\n",
    "    tokens = tokenizer.encode_plus(sentence1, sentence2, return_tensors=\"pt\")\n",
    "    tokens_tensor = tokens[\"input_ids\"].to(device)\n",
    "    segment_tensor = tokens[\"token_type_ids\"].to(device)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        # Assuming the model returns NSP predictions first\n",
    "        nsp_prediction, _ = model(tokens_tensor, segment_tensor)\n",
    "        # Select the first element (first sequence) of the logits tensor\n",
    "        first_logits = nsp_prediction[0].unsqueeze(0)  # Adds an extra dimension, making it [1, 2]\n",
    "        logits = torch.softmax(first_logits, dim=1)\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Interpret the prediction\n",
    "    return \"Second sentence follows the first\" if prediction == 1 else \"Second sentence does not follow the first\"\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"The cat sat on the mat.\"\n",
    "sentence2 = \"It was a sunny day\"\n",
    "\n",
    "print(predict_nsp(sentence1, sentence2, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function is defined to perform Masked Language Modeling (MLM) using a pretrained BERT model. The function operates as follows:\n",
    "\n",
    "1. Tokenization: The input sentence is tokenized using the tokenizer and converted into token IDs, including the special tokens. The tokenized sentence is stored in the `tokens_tensor` variable.\n",
    "\n",
    "2. Segment Labels: Dummy segment labels filled with zeros are created and stored as `segment_labels`.\n",
    "\n",
    "3. Prediction: The BERT model is used to make predictions by passing the token tensor and segment labels as input. The MLM logits are extracted as `predictions`.\n",
    "\n",
    "4. Mask Token Index: The position of the [MASK] token is identified using the `nonzero` method and stored in the `mask_token_index` variable. Note that all tokens except the mask token are zero-padded.\n",
    "\n",
    "5. Predicted Index: The predicted index for the [MASK] token is obtained by taking the argmax of the MLM logits at the corresponding position.\n",
    "\n",
    "6. Token Conversion: The predicted index is converted back to a token using the `convert_ids_to_tokens` method of the tokenizer.\n",
    "\n",
    "7. Replaced Sentence: The original sentence is replaced with the predicted token at the position of the [MASK] token, resulting in the predicted sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sat on the [UNK].\n"
     ]
    }
   ],
   "source": [
    "def predict_mlm(sentence, model, tokenizer):\n",
    "    # Tokenize the input sentence and convert to token IDs, including special tokens\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens_tensor = inputs.input_ids\n",
    "\n",
    "    # Create dummy segment labels filled with zeros, assuming it's needed by your model\n",
    "    segment_labels = torch.zeros_like(tokens_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model, now correctly handling the output tuple\n",
    "        output_tuple = model(tokens_tensor, segment_labels)\n",
    "\n",
    "        # Assuming the second element of the tuple contains the MLM logits\n",
    "        predictions = output_tuple[1]  # Adjusted based on your model's output\n",
    "\n",
    "        # Identify the position of the [MASK] token\n",
    "        mask_token_index = (tokens_tensor == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # Get the predicted index for the [MASK] token from the MLM logits\n",
    "        predicted_index = torch.argmax(predictions[0, mask_token_index.item(), :], dim=-1)\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index.item()])[0]\n",
    "\n",
    "        # Replace [MASK] in the original sentence with the predicted token\n",
    "        predicted_sentence = sentence.replace(tokenizer.mask_token, predicted_token, 1)\n",
    "\n",
    "    return predicted_sentence\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sentence = \"The cat sat on the [MASK].\"\n",
    "print(predict_mlm(sentence, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratualtions! You've just learnt how to create a tiny BERT model and trained it for a few epochs. for the model to generate accurate result, you will need to train it on huge datasets for more epochs and maybe increase the model size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "Now you have preatined our own Baby BERT and evaluated it now to see the difference lets use a pretrained BERT model to predict for few examples as above. \n",
    "\n",
    "**Note: Since the actual BERT is trained with a lot more data and epochs, its performance should be much better than the model you just built.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Next Sentence Prediction (NSP) with BERT\n",
    "\n",
    "1. **Load the BERT pretrained model**: Import `BertForPreTraining` and `BertTokenizer` from `transformers`, and load the 'bert-base-uncased' pretrained model and tokenizer.\n",
    "2. **Prepare text input**: Encode a pair of sentences using the loaded tokenizer.\n",
    "3. **Perform NSP**: Pass the encoded input through the model and interpret the `seq_relationship_logits` to determine if the model predicts the sentences as consecutive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1971d8d6d4b4f929842eb383c78bf64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model thinks these sentences are NOT consecutive.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForPreTraining, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Write your code here\n",
    "# Load pretrained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pretrained model (weights)\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "# Prepare text pair for NSP\n",
    "text_1 = \"The cat sat on the mat\"\n",
    "text_2 = \"It was a sunny day\"\n",
    "# Encode text\n",
    "inputs = tokenizer(text_1, text_2, return_tensors=\"pt\")\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, next_sentence_label=torch.LongTensor([1]))\n",
    "    nsp_logits = outputs.seq_relationship_logits\n",
    "\n",
    "# Interpret the result for NSP\n",
    "if torch.argmax(nsp_logits, dim=-1).item() == 0:\n",
    "    print(\"The model thinks these sentences are NOT consecutive.\")\n",
    "else:\n",
    "    print(\"The model thinks these sentences are consecutive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Load pretrained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pretrained model (weights)\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "# Prepare text pair for NSP\n",
    "text_1 = \"The cat sat on the mat\"\n",
    "text_2 = \"It was a sunny day\"\n",
    "# Encode text\n",
    "inputs = tokenizer(text_1, text_2, return_tensors=\"pt\")\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, next_sentence_label=torch.LongTensor([1]))\n",
    "    nsp_logits = outputs.seq_relationship_logits\n",
    "\n",
    "# Interpret the result for NSP\n",
    "if torch.argmax(nsp_logits, dim=-1).item() == 0:\n",
    "    print(\"The model thinks these sentences are NOT consecutive.\")\n",
    "else:\n",
    "    print(\"The model thinks these sentences are consecutive.\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Masked Language Modeling (MLM) with BERT\n",
    "1. **Initialize the model and tokenizer**:\n",
    "   Load `BertForPreTraining` and `BertTokenizer` from the `transformers` library using the 'bert-base-uncased' model.\n",
    "2. **Prepare the masked sentence**:\n",
    "   Write a sentence and replace one word with `[MASK]`. For example, \"The capital of France is [MASK].\"\n",
    "3. **Tokenize and predict**:\n",
    "   Tokenize the masked sentence with `BertTokenizer.` Then, input it to `BertForPreTraining` and use the `prediction_logits` to find the most probable token that fits the mask.\n",
    "4. **Display the prediction**:\n",
    "   Convert the predicted token ID back to a token string and print out the predicted word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: ['paris']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForPreTraining, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Write your code here\n",
    "from transformers import BertForPreTraining, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pretrained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pretrained model (weights)\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare text with masked token\n",
    "masked_text = \"The capital of France is [MASK].\"\n",
    "# Tokenize and prepare for the model: Convert to tokens and add special tokens\n",
    "input_ids = tokenizer(masked_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids)\n",
    "    predictions = outputs.prediction_logits\n",
    "\n",
    "# Confirm we were able to predict 'Paris' as the masked token\n",
    "predicted_index = torch.argmax(predictions[0, input_ids[0] == tokenizer.mask_token_id]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "\n",
    "print(f\"Predicted token: {predicted_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "from transformers import BertForPreTraining, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pretrained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pretrained model (weights)\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare text with masked token\n",
    "masked_text = \"The capital of France is [MASK].\"\n",
    "# Tokenize and prepare for the model: Convert to tokens and add special tokens\n",
    "input_ids = tokenizer(masked_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids)\n",
    "    predictions = outputs.prediction_logits\n",
    "\n",
    "# Confirm we were able to predict 'Paris' as the masked token\n",
    "predicted_index = torch.argmax(predictions[0, input_ids[0] == tokenizer.mask_token_id]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "\n",
    "print(f\"Predicted token: {predicted_token}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo) has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "[Ashutosh Sagar](https://www.linkedin.com/in/ashutoshsagar/) is completing his MS in CS at Dalhousie University. He has previous experience in Natural Language Processing and as a Data Scientist.\n",
    "\n",
    "## Contributors\n",
    "\n",
    "[Fateme Akbari](https://www.linkedin.com/in/fatemeakbari/) is a Ph.D. candidate in Information Systems at McMaster University with demonstrated research experience in Machine Learning and NLP.\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "- [Mastering BERT Model: Building it from Scratch with Pytorch](https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "0b25207227742137e14bad52642396cb60cccc974d957d37db77492b29e7d25f"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
